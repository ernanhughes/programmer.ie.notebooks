{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\programmer.ie.notebooks\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5691652297973633\n",
      "Epoch 2, Loss: 0.34353339672088623\n",
      "Epoch 3, Loss: 0.11656033992767334\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained model for reward estimation\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "\n",
    "# Sample training data (prompt-response pairs with human-labeled scores)\n",
    "data = [\n",
    "    (\"What is AI?\", \"AI stands for Artificial Intelligence.\", 0.9),\n",
    "    (\"What is deep learning?\", \"Deep learning is a subset of machine learning.\", 0.8),\n",
    "    (\"Tell me a joke.\", \"Why did the chicken cross the road?\", 0.6)\n",
    "]\n",
    "\n",
    "# Convert data into tensors\n",
    "inputs = tokenizer([d[1] for d in data], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor([d[2] for d in data]).unsqueeze(1)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.AdamW(reward_model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "reward_model.train()\n",
    "for epoch in range(3):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = reward_model(**inputs).logits\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1...\n",
      "Epoch 1 Fine-tuning loss: 11.924062728881836\n",
      "Epoch 2 Fine-tuning loss: 11.158195495605469\n",
      "Epoch 3 Fine-tuning loss: 10.423539161682129\n",
      "Iteration 2...\n",
      "Epoch 1 Fine-tuning loss: 8.566413879394531\n",
      "Epoch 2 Fine-tuning loss: 8.518267631530762\n",
      "Epoch 3 Fine-tuning loss: 7.131834506988525\n",
      "Iteration 3...\n",
      "Epoch 1 Fine-tuning loss: 6.632737159729004\n",
      "Epoch 2 Fine-tuning loss: 5.000913143157959\n",
      "Epoch 3 Fine-tuning loss: 4.756820201873779\n",
      "Iteration 4...\n",
      "Epoch 1 Fine-tuning loss: 4.7517194747924805\n",
      "Epoch 2 Fine-tuning loss: 4.390162944793701\n",
      "Epoch 3 Fine-tuning loss: 3.733025312423706\n",
      "Iteration 5...\n",
      "Epoch 1 Fine-tuning loss: 2.687434673309326\n",
      "Epoch 2 Fine-tuning loss: 1.3824565410614014\n",
      "Epoch 3 Fine-tuning loss: 1.3723318576812744\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import random\n",
    "\n",
    "# Load a small generative model (e.g., GPT-2)\n",
    "generative_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "def generate_samples(prompt, num_samples=5):\n",
    "    \"\"\"Generate multiple responses for a given prompt.\"\"\"\n",
    "    return [f\"Generated response {i} for {prompt}\" for i in range(num_samples)]\n",
    "\n",
    "def rank_samples(samples):\n",
    "    \"\"\"Rank samples using the reward model.\"\"\"\n",
    "    inputs = tokenizer(samples, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_model(**inputs).logits.squeeze()\n",
    "    return [samples[i] for i in torch.argsort(rewards, descending=True)]\n",
    "\n",
    "def fine_tune_model(model, dataset):\n",
    "    \"\"\"Fine-tune the generative model on high-reward samples.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for sample in dataset:\n",
    "            inputs = tokenizer(sample, return_tensors=\"pt\")\n",
    "            labels = inputs[\"input_ids\"].clone()\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1} Fine-tuning loss: {loss.item()}\")\n",
    "\n",
    "# RAFT loop\n",
    "for iteration in range(5):\n",
    "    print(f\"Iteration {iteration+1}...\")\n",
    "    prompt = \"What is artificial intelligence?\"\n",
    "    samples = generate_samples(prompt)\n",
    "    ranked_samples = rank_samples(samples)[:3]  # Select top-3 responses\n",
    "    fine_tune_model(generative_model, ranked_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Score: 27.507169723510742\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def compute_reward(image, text):\n",
    "    \"\"\"Compute the similarity score between an image and text.\"\"\"\n",
    "    inputs = clip_processor(text=[text], images=image, return_tensors=\"pt\")\n",
    "    scores = clip_model(**inputs).logits_per_image\n",
    "    return scores.item()\n",
    "\n",
    "# Example: Evaluating a generated image\n",
    "image_path = \"anime_boy.png\"\n",
    "image = Image.open(image_path)\n",
    "reward_score = compute_reward(image, \"A high-quality anime-style portrait\")\n",
    "print(\"Reward Score:\", reward_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
