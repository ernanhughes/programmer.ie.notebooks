{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d665e4d",
   "metadata": {},
   "source": [
    "+++\n",
    "date = '2025-02-06T08:44:07Z'\n",
    "draft = true\n",
    "title = 'Writing Neural Networks with PyTorch'\n",
    "categories = ['Deep Learning', 'PyTorch', 'Neural Networks', 'AI']\n",
    "tags = ['pytorch', 'neural networks', 'deep learning', 'AI', 'machine learning']\n",
    "+++\n",
    "\n",
    "### Summary\n",
    "\n",
    "This post provides a practical guide to building common neural network architectures using PyTorch. We'll explore `feedforward` networks, `convolutional` neural networks (CNNs), `recurrent` neural networks (RNNs), `LSTM`s, `transformers`, `autoencoders`, and `GAN`s, along with code examples and explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93636d4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### **1. Understanding PyTorch's Neural Network Module**\n",
    "\n",
    "PyTorch provides the `torch.nn` module to build neural networks. \n",
    "It provides classes for defining layers, [activation functions]({{< relref \"post/activation.md\" >}}), and loss functions, making it easy to create and manage complex network architectures in a structured way.\n",
    "\n",
    "#### **Key Components:**\n",
    "* `torch.nn.Module`: The base class for all neural network models.\n",
    "* `torch.nn.Linear`: A fully connected (dense) layer.\n",
    "* `torch.nn.ReLU`, `torch.nn.Sigmoid`, `torch.nn.Tanh`, `torch.nn.Softmax`: Common [activation functions]({{< relref \"post/activation.md\" >}}).\n",
    "* `torch.optim`: Optimizers for training.\n",
    "* `torch.nn.functional` (often imported as `F`): Contains activation functions, loss functions, and other utility functions.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Creating a Simple Feedforward Neural Network**\n",
    "\n",
    "Let's build a basic neural network with PyTorch using `torch.nn.Module`.\n",
    "\n",
    "#### **Step 1: Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4907a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb2a8f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Step 2: Define the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b911874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef24973",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Step 3: Create a Model Instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0b5e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 5\n",
    "output_size = 2\n",
    "\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "print(model) # Print the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a2db3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### **3. Training the Neural Network**\n",
    "\n",
    "To train the neural network, we need:\n",
    "- A **loss function** (e.g., Mean Squared Error for regression, Cross Entropy for classification).\n",
    "- An **optimizer** (e.g., Stochastic Gradient Descent, Adam).\n",
    "- A **training loop** to update model weights.\n",
    "\n",
    "#### **Step 1: Define Loss Function and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7538ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # For classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641e55b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Step 2: Generate Some Dummy Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "702cf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input and target data\n",
    "x_train = torch.randn(100, input_size)  # 100 samples, 10 features each\n",
    "y_train = torch.randint(0, output_size, (100,))  # 100 labels (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06341ab0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Step 3: Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba65d561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7435\n",
      "Epoch [20/100], Loss: 0.7355\n",
      "Epoch [30/100], Loss: 0.7286\n",
      "Epoch [40/100], Loss: 0.7226\n",
      "Epoch [50/100], Loss: 0.7174\n",
      "Epoch [60/100], Loss: 0.7127\n",
      "Epoch [70/100], Loss: 0.7084\n",
      "Epoch [80/100], Loss: 0.7041\n",
      "Epoch [90/100], Loss: 0.6999\n",
      "Epoch [100/100], Loss: 0.6956\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    outputs = model(x_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75997989",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **4. Evaluating the Model**\n",
    "\n",
    "After training, we need to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c90c31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    test_inputs = torch.randn(10, input_size)\n",
    "    test_outputs = model(test_inputs)\n",
    "    predicted_labels = torch.argmax(test_outputs, dim=1)\n",
    "    print(\"Predicted Labels:\", predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8141f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **5. Using Activation Functions**\n",
    "\n",
    "PyTorch provides multiple activation functions. Hereâ€™s an example of how to use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709a079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ActivationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f27499",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### **6. Saving and Loading Models**\n",
    "\n",
    "Saving models is essential for reusing trained networks.\n",
    "\n",
    "#### **Saving the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea804065",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907df396",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Loading the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b281dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNN(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()  # Set model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28a83e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece30b9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Different types of Neural Networks\n",
    "\n",
    "Neural networks can be categorized based on their **architecture, functionality, and use cases**. Here are the main types:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. Feedforward Neural Networks (FNNs)**\n",
    "- The simplest type of neural network.\n",
    "- Information moves in one direction: from input to output (no loops).\n",
    "- Used for tasks like **classification** and **regression**.\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "766bf6f3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNN(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FeedforwardNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = FeedforwardNN(input_size, hidden_size, output_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d8554b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **2. Convolutional Neural Networks (CNNs)**\n",
    "- Designed for image processing and computer vision tasks.\n",
    "- Uses convolutional layers to **extract spatial features** from images.\n",
    "- Includes pooling layers to reduce dimensionality.\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4300f7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=3136, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 10)  # Fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d7e9d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **3. Recurrent Neural Networks (RNNs)**\n",
    "- Designed for **sequential data** (e.g., time series, natural language processing).\n",
    "- Uses **hidden states** to retain information from previous steps.\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa622264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(10, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last time step output\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size=10, hidden_size=5, output_size=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3738d2e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **4. Long Short-Term Memory Networks (LSTMs)**\n",
    "- A special type of RNN designed to handle **long-term dependencies**.\n",
    "- Uses **gates (input, forget, and output)** to regulate information flow.\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6978d1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(10, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = LSTM(input_size=10, hidden_size=5, output_size=2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c94e3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **5. Transformers**\n",
    "- Used in **Natural Language Processing (NLP)** (e.g., BERT, GPT).\n",
    "- Replaces RNNs/LSTMs with **self-attention mechanisms**.\n",
    "- Scales well to large datasets.\n",
    "\n",
    "**Example using PyTorch's `transformers` library:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad103e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\projects\\programmer.ie.notebooks\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSdpaSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc1c83b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **6. Autoencoders**\n",
    "- Used for **dimensionality reduction, anomaly detection, and generative models**.\n",
    "- Consists of an **encoder** (compressing input) and a **decoder** (reconstructing input).\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a769326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (decoder): Linear(in_features=128, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Linear(784, 128)\n",
    "        self.decoder = nn.Linear(128, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.encoder(x))\n",
    "        x = torch.sigmoid(self.decoder(x))\n",
    "        return x\n",
    "    \n",
    "model = Autoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89dcea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "#### **7. Generative Adversarial Networks (GANs)**\n",
    "- Used for **image generation** (e.g., DeepFake, AI art).\n",
    "- Composed of:\n",
    "  - **Generator** (creates fake samples).\n",
    "  - **Discriminator** (determines real vs. fake samples).\n",
    "\n",
    "**Example in PyTorch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c46f061b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (fc): Linear(in_features=100, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Linear(100, 784)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.fc(x))\n",
    "    \n",
    "\n",
    "model = Generator()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338204d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Summary of Neural Network Types**\n",
    "| Type | Best For |\n",
    "|------|---------|\n",
    "| **Feedforward NN** | General tasks, structured data |\n",
    "| **CNN** | Image recognition, computer vision |\n",
    "| **RNN** | Sequential data, speech processing |\n",
    "| **LSTM** | Long-term memory retention (e.g., chatbots) |\n",
    "| **Transformers** | NLP, large-scale text tasks |\n",
    "| **Autoencoders** | Dimensionality reduction, anomaly detection |\n",
    "| **GANs** | Image generation, unsupervised learning |\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
